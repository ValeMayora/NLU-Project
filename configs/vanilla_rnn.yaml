model:
  vocab_size: null         
  embedding_dim: 300
  hidden_dim: 200
  num_layers: 1
  dropout: 0.3

training:
  learning_rate: 0.1       
  batch_size: 32
  n_epochs: 40
  patience_init: 5
  clip: 5

output:
  save_path: "./results/"
  log_path: "./logs/"
