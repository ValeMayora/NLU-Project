model:
  vocab_size: null
  embedding_dim: 300
  hidden_dim: 200
  num_layers: 1
  use_dropout_embed: true
  use_dropout_output: true 
  dropout_embed: 0.1
  dropout_output: 0.1

training:
  optimizer: "adamw"      # Options: "SGD", "AdamW"
  learning_rate: 0.001
  batch_size: 20
  epochs: 100

data:
  sequence_length: 30

output:
  save_path: "./results/lstm/"
  log_path: "./logs/lstm/"

